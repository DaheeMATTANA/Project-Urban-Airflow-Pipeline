# Platform

This folder contains the **data engineering platform layer** of Urban Airflow Pipeline.

It includes all components related to infrastructure, orchestration, and ingestion.

---

## üìÇ Structure
- **airflow/** ‚Üí DAGs, plugins, and Docker config to run Airflow locally.
- **pipelines/** ‚Üí Python code for ingestion & streaming (batch APIs, producers/consumers).
- **minio/** ‚Üí Local object storage (S3-compatible) replacing GCS buckets.
- **redpanda/** ‚Üí Local Kafka-compatible broker for streaming.

---

## üöÄ Goals
- Provide a **local-first stack** that mirrors cloud-native architecture.
- Enable **streaming ingestion** (Redpanda) and **batch pipelines** (Airflow).
- Store data in **bronze/silver/gold/logs** buckets via MinIO.

---

## Airflow DAGs

### Healthcheck DAG

Un DAG trivial (`healthcheck`) est fourni pour v√©rifier que le scheduler fonctionne.

**UI :**
1. Dans la liste des DAGs, activer `healthcheck`
2. Cliquer sur le bouton ‚ñ∂Ô∏è *Trigger DAG*
3. V√©rifier que le run obtient un ‚úÖ vert dans les 30 secondes

**CLI (dans le container scheduler) :**
```bash
# Lancer le DAG
docker exec -it airflow-scheduler \
  airflow dags trigger healthcheck

# V√©rifier l‚Äô√©tat du dernier run
docker exec -it airflow-scheduler \
  airflow dags state healthcheck $(date +%Y-%m-%d)T00:00:00+00:00